- [x] add basic WGPU tensor support
  - referenced: https://github.com/0hq/WebGPT/blob/main/condensed/condensed.js
  - referenced: https://github.com/burn-rs/burn/blob/main/burn-wgpu/src/template/matmul/naive.wgsl
  - referenced: https://github.com/huggingface/candle/blob/main/candle-core/src/cpu_backend.rs
- [x] factor out TensorArithmetic
- [x] revise the dot product attention primitive
  - reference mlx: https://github.com/simonw/llm-mlx-llama/blob/main/llm_mlx_llama.py#L81
  - learn the kv cache layout
- [x] use uniform to store the meta buffers
- [x] a better gemv
- [x] refactor the buf code
- [x] q8_0 dot product
- [x] compare the matmul q8_0 FLOPS between ggml and crabml
  - [x] aligh the performance on dot prod: try using manual neon instructions
- [ ] find the performance difference between ggml
  - [ ] record the time spent on different operators like matmul, etc.
  - [ ] optimize the performance of MHA
- [ ] q8 quantization on webgpu
  - [ ] add dequantize in CpuTensor
