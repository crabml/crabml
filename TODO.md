- [x] add basic WGPU tensor support
  - referenced: https://github.com/0hq/WebGPT/blob/main/condensed/condensed.js
  - referenced: https://github.com/burn-rs/burn/blob/main/burn-wgpu/src/template/matmul/naive.wgsl
  - referenced: https://github.com/huggingface/candle/blob/main/candle-core/src/cpu_backend.rs
- [x] factor out TensorArithmetic
- [x] revise the dot product attention primitive
  - reference mlx: https://github.com/simonw/llm-mlx-llama/blob/main/llm_mlx_llama.py#L81
  - learn the kv cache layout
- [x] use uniform to store the meta buffers
- [x] a better gemv
- [x] refactor the buf code
- [ ] seek again why ggml is faster on inferencing 3B model with CPU
  - [ ] plot the time duration during matmul and batch matmul
- [ ] q8 quantization on webgpu
  - [ ] add dequantize in CpuTensor
  - [ ] refactor the CPU side: dequantize these on loading: token_embedding_table, rms_att_weight, rms_ffn_weight, rms_final_weight, wcls
  - [ ] all the other are only matmul
- [ ] f16 activations
